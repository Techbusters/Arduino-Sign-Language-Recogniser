# Arduino-Sign-Language-Recogniser

A Convolution Neural Network model for translating American sign language using Arduino Uno

Purpose:


This project is objected to reduce the communication gap between normal persons and so called deaf and dumb persons. The main communication method for the later persons is the standard sign language. The interpretation of their language requires years of training and experience and is often ambiguous to normal persons. In this context, our project comes into picture.

Structure:


It contains two parts: one is the arduino based hand motion detector and the other is deep learning model using keras.
The former one is integrated for the ease of hand-eye coordination. The camera will follw the hand motion and rotate to the direction of the hand, so that the user does not have to remain stationary and perform gestures in a limited boundary.
The second part is the heart of the project. It uses Keras backend for predicting the class of the gesture captured by the web-cam, using some pre-trained model.The captured image is processed using OpenCV framework.The model is trained on a dataset which was generated by us. The output is shown as text in a GUI made using Tkinter.


File structures:

The actual trained model is available in MyModelDataset1

The video capturing code is in Modelpredict.py

The arduino code is inside MotionSensorCamera.ino
